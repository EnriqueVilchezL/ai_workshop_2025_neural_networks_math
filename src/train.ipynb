{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/EnriqueVilchezL/ai_workshop_2025_neural_networks_math/blob/main/src/train.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "Neural networks are powerful machine learning models inspired by the human brain. They are widely used for tasks such as image recognition, natural language processing, and pattern detection. Training a neural network involves adjusting its weights using a dataset to minimize prediction errors.\n",
    "\n",
    "In this workshop, we will train a neural network using the MNIST dataset, a collection of handwritten digits. We will go through the essential steps required to build, train, and evaluate a neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Modules and Initial Configuration\n",
    "\n",
    "The necessary libraries are imported to build and train the model. Additionally, a seed is set to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/EnriqueVilchezL/ai_workshop_2025_neural_networks_math\n",
    "%pip install numpy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ai_workshop_2025_neural_networks_math/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.activation import *\n",
    "from network.layer import *\n",
    "from network.loss import *\n",
    "from network.optimizer import *\n",
    "from network.sequential import *\n",
    "from network.metric import *\n",
    "\n",
    "import numpy as np\n",
    "import mnist.mnist as mnist\n",
    "\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Training Function\n",
    "\n",
    "The function responsible for training the model using the training and validation datasets is defined. This function performs the following tasks:\n",
    "- Splits data into batches.\n",
    "- Performs forward propagation.\n",
    "- Computes loss and metric.\n",
    "- Performs backpropagation.\n",
    "- Updates model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: Sequential,\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    Y_val: np.ndarray,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    optimizer: Optimizer,\n",
    "    loss_function: Loss,\n",
    "    metric_function : Metric\n",
    ") -> None:\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        metric = 0\n",
    "        batches_steps = range(0, len(X), batch_size)\n",
    "        total_steps = len(batches_steps)\n",
    "        shuffled_indexes = np.random.permutation(len(X))\n",
    "        X = X[shuffled_indexes]\n",
    "        Y = Y[shuffled_indexes]\n",
    "        for i in batches_steps:\n",
    "            x_batch = X[i:i+batch_size]\n",
    "            y_batch = Y[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_hat = model.forward({'X' : x_batch})\n",
    "            # Compute loss\n",
    "            batch_loss = loss_function.forward({'Y' : y_batch, 'Y_hat' : y_hat})\n",
    "            batch_metric = metric_function.compute({'Y' : y_batch, 'Y_hat' : y_hat})\n",
    "            # Compute gradients\n",
    "            loss_function.backward()\n",
    "            # Backward pass\n",
    "            model.backward({'dY' : loss_function.gradients['dY_hat']})\n",
    "            # Update parameters\n",
    "            optimizer.update(model)\n",
    "            # Accumulate batch loss mean\n",
    "            loss += batch_loss.mean()\n",
    "            metric += batch_metric\n",
    "        \n",
    "        y_hat = model.forward({'X': X_val})\n",
    "        val_loss = loss_function.forward({'Y': Y_val, 'Y_hat': y_hat}).mean()\n",
    "        val_metric = metric_function.compute({'Y': Y_val, 'Y_hat': y_hat})\n",
    "\n",
    "        print(f\"Train ==> Epoch {epoch+1}/{epochs} loss: {loss/total_steps} accuracy: {metric/total_steps}\")\n",
    "        print(f\"Validation ==> Epoch {epoch+1}/{epochs} loss: {val_loss} accuracy: {val_metric}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Model Configuration\n",
    "\n",
    "In this section, the MNIST dataset is loaded and values are normalized. Additionally, the neural network architecture, loss function, and optimizer are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mnist\n",
    "x_train, y_train, x_test, y_test = mnist.load('mnist/mnist.pkl')\n",
    "\n",
    "# Normalize data\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "# Add an extra dimension\n",
    "y_test = np.eye(10)[y_test].squeeze()\n",
    "y_train = np.eye(10)[y_train].squeeze()\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(784, 16),\n",
    "    Sigmoid(),\n",
    "    Dense(16, 16),\n",
    "    Sigmoid(),\n",
    "    Dense(16, 10),\n",
    "    Softmax()\n",
    "])\n",
    "\n",
    "optimizer = StochasticGradientDescent(learning_rate=0.001)\n",
    "loss = CategoricalCrossEntropy()\n",
    "metric = Accuracy()\n",
    "\n",
    "try:\n",
    "    train(model, x_train, y_train, x_test, y_test, 100, 64, optimizer, loss, metric)\n",
    "    model.save(\"model.pkl\")\n",
    "except KeyboardInterrupt:\n",
    "    model.save(\"model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
